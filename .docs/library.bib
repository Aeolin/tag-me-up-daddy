
@misc{tsaptsinos_lyrics-based_2017,
	title = {Lyrics-{Based} {Music} {Genre} {Classification} {Using} a {Hierarchical} {Attention} {Network}},
	url = {http://arxiv.org/abs/1707.04678},
	doi = {10.48550/arXiv.1707.04678},
	abstract = {Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure - in which words combine to form lines, lines form segments, and segments form a complete song - we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Tsaptsinos, Alexandros},
	month = jul,
	year = {2017},
	note = {arXiv:1707.04678 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\flori\\Zotero\\storage\\HSYWKV35\\Tsaptsinos - 2017 - Lyrics-Based Music Genre Classification Using a Hi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\flori\\Zotero\\storage\\NI22386E\\1707.html:text/html},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\flori\\Zotero\\storage\\2ZSLT5QI\\Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\flori\\Zotero\\storage\\LA3T8RS7\\2310.html:text/html},
}

@misc{noauthor_dcase2019_nodate,
	title = {{DCASE2019} {Challenge} - {DCASE}},
	url = {https://dcase.community/challenge2019/index},
	urldate = {2024-04-24},
	file = {DCASE2019 Challenge - DCASE:C\:\\Users\\flori\\Zotero\\storage\\2QXCPE4Z\\index.html:text/html},
}

@article{xu_general_2019,
	title = {General audio tagging with ensembling convolutional neural network and statistical features},
	volume = {145},
	issn = {0001-4966, 1520-8524},
	url = {http://arxiv.org/abs/1810.12832},
	doi = {10.1121/1.5111059},
	abstract = {Audio tagging aims to infer descriptive labels from audio clips. Audio tagging is challenging due to the limited size of data and noisy labels. In this paper, we describe our solution for the DCASE 2018 Task 2 general audio tagging challenge. The contributions of our solution include: We investigated a variety of convolutional neural network architectures to solve the audio tagging task. Statistical features are applied to capture statistical patterns of audio features to improve the classification performance. Ensemble learning is applied to ensemble the outputs from the deep classifiers to utilize complementary information. a sample re-weight strategy is employed for ensemble training to address the noisy label problem. Our system achieves a mean average precision (mAP@3) of 0.958, outperforming the baseline system of 0.704. Our system ranked the 1st and 4th out of 558 submissions in the public and private leaderboard of DCASE 2018 Task 2 challenge. Our codes are available at https://github.com/Cocoxili/DCASE2018Task2/.},
	number = {6},
	urldate = {2024-04-24},
	journal = {The Journal of the Acoustical Society of America},
	author = {Xu, Kele and Zhu, Boqing and Kong, Qiuqiang and Mi, Haibo and Ding, Bo and Wang, Dezhi and Wang, Huaimin},
	month = jun,
	year = {2019},
	note = {arXiv:1810.12832 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {EL521--EL527},
	file = {arXiv Fulltext PDF:C\:\\Users\\flori\\Zotero\\storage\\HG7Z6P8C\\Xu et al. - 2019 - General audio tagging with ensembling convolutiona.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\flori\\Zotero\\storage\\NXPYKXXH\\1810.html:text/html},
}

@misc{fonseca_audio_2020,
	title = {Audio tagging with noisy labels and minimal supervision},
	url = {http://arxiv.org/abs/1906.02975},
	doi = {10.48550/arXiv.1906.02975},
	abstract = {This paper introduces Task 2 of the DCASE2019 Challenge, titled "Audio tagging with noisy labels and minimal supervision". This task was hosted on the Kaggle platform as "Freesound Audio Tagging 2019". The task evaluates systems for multi-label audio tagging using a large set of noisy-labeled data, and a much smaller set of manually-labeled data, under a large vocabulary setting of 80 everyday sound classes. In addition, the proposed dataset poses an acoustic mismatch problem between the noisy train set and the test set due to the fact that they come from different web audio sources. This can correspond to a realistic scenario given by the difficulty in gathering large amounts of manually labeled data. We present the task setup, the FSDKaggle2019 dataset prepared for this scientific evaluation, and a baseline system consisting of a convolutional neural network. All these resources are freely available.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Fonseca, Eduardo and Plakal, Manoj and Font, Frederic and Ellis, Daniel P. W. and Serra, Xavier},
	month = jan,
	year = {2020},
	note = {arXiv:1906.02975 [cs, eess, stat]
version: 4},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\flori\\Zotero\\storage\\FIN8DL9F\\Fonseca et al. - 2020 - Audio tagging with noisy labels and minimal superv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\flori\\Zotero\\storage\\9ACGHT54\\1906.html:text/html},
}

@misc{schmid_efficient_2023,
	title = {Efficient {Large}-scale {Audio} {Tagging} via {Transformer}-to-{CNN} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2211.04772},
	doi = {10.48550/arXiv.2211.04772},
	abstract = {Audio Spectrogram Transformer models rule the field of Audio Tagging, outrunning previously dominating Convolutional Neural Networks (CNNs). Their superiority is based on the ability to scale up and exploit large-scale datasets such as AudioSet. However, Transformers are demanding in terms of model size and computational requirements compared to CNNs. We propose a training procedure for efficient CNNs based on offline Knowledge Distillation (KD) from high-performing yet complex transformers. The proposed training schema and the efficient CNN design based on MobileNetV3 results in models outperforming previous solutions in terms of parameter and computational efficiency and prediction performance. We provide models of different complexity levels, scaling from low-complexity models up to a new state-of-the-art performance of .483 mAP on AudioSet. Source Code available at: https://github.com/fschmid56/EfficientAT},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Schmid, Florian and Koutini, Khaled and Widmer, Gerhard},
	month = jun,
	year = {2023},
	note = {arXiv:2211.04772 [cs, eess]
version: 3},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\flori\\Zotero\\storage\\6NKAS3HF\\Schmid et al. - 2023 - Efficient Large-scale Audio Tagging via Transforme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\flori\\Zotero\\storage\\MX3NCG9K\\2211.html:text/html},
}

@misc{pons_musicnn_2019,
	title = {musicnn: {Pre}-trained convolutional neural networks for music audio tagging},
	shorttitle = {musicnn},
	url = {http://arxiv.org/abs/1909.06654},
	doi = {10.48550/arXiv.1909.06654},
	abstract = {Pronounced as "musician", the musicnn library contains a set of pre-trained musically motivated convolutional neural networks for music audio tagging: https://github.com/jordipons/musicnn. This repository also includes some pre-trained vgg-like baselines. These models can be used as out-of-the-box music audio taggers, as music feature extractors, or as pre-trained models for transfer learning. We also provide the code to train the aforementioned models: https://github.com/jordipons/musicnn-training. This framework also allows implementing novel models. For example, a musically motivated convolutional neural network with an attention-based output layer (instead of the temporal pooling layer) can achieve state-of-the-art results for music audio tagging: 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset --- and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Pons, Jordi and Serra, Xavier},
	month = sep,
	year = {2019},
	note = {arXiv:1909.06654 [cs, eess]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\flori\\Zotero\\storage\\47RGU42F\\Pons und Serra - 2019 - musicnn Pre-trained convolutional neural networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\flori\\Zotero\\storage\\SFH7N8H8\\1909.html:text/html},
}

@article{mckinney_features_2003,
	title = {Features for {Audio} and {Music} {Classification}},
	abstract = {Four audio feature sets are evaluated in their ability to classify five general audio classes and seven popular music genres. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for both music and audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.},
	author = {McKinney, Martin and Breebaart, Jeroen and (wy, Prof},
	month = nov,
	year = {2003},
	file = {Full Text PDF:C\:\\Users\\flori\\Zotero\\storage\\7SIQYI9Y\\McKinney et al. - 2003 - Features for Audio and Music Classification.pdf:application/pdf},
}

@inproceedings{fell_lyrics-based_2014,
	address = {Dublin, Ireland},
	title = {Lyrics-based {Analysis} and {Classification} of {Music}},
	url = {https://aclanthology.org/C14-1059},
	urldate = {2024-04-24},
	booktitle = {Proceedings of {COLING} 2014, the 25th {International} {Conference} on {Computational} {Linguistics}: {Technical} {Papers}},
	publisher = {Dublin City University and Association for Computational Linguistics},
	author = {Fell, Michael and Sporleder, Caroline},
	editor = {Tsujii, Junichi and Hajic, Jan},
	month = aug,
	year = {2014},
	pages = {620--631},
	file = {Full Text PDF:C\:\\Users\\flori\\Zotero\\storage\\XSEBG8HS\\Fell und Sporleder - 2014 - Lyrics-based Analysis and Classification of Music.pdf:application/pdf},
}

@inproceedings{smith_your_2012,
	address = {Dunedin, New Zealand},
	title = {In {Your} {Eyes}: {Identifying} {Clichés} in {Song} {Lyrics}},
	shorttitle = {In {Your} {Eyes}},
	url = {https://aclanthology.org/U12-1012},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the {Australasian} {Language} {Technology} {Association} {Workshop} 2012},
	author = {Smith, Alex G. and Zee, Christopher X. S. and Uitdenbogerd, Alexandra L.},
	editor = {Cook, Paul and Nowson, Scott},
	month = dec,
	year = {2012},
	pages = {88--96},
	file = {Full Text PDF:C\:\\Users\\flori\\Zotero\\storage\\QMHE4XQ3\\Smith et al. - 2012 - In Your Eyes Identifying Clichés in Song Lyrics.pdf:application/pdf},
}

@misc{noauthor_notebooksmistral-finetune-own-dataipynb_nodate,
	title = {notebooks/mistral-finetune-own-data.ipynb at main · brevdev/notebooks},
	url = {https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb},
	abstract = {Contribute to brevdev/notebooks development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-04-24},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\flori\\Zotero\\storage\\7NSJ6HTS\\mistral-finetune-own-data.html:text/html},
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCx6jZU3m11"
   },
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> â€¢\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> â€¢\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> â€¢\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Fine-tuning Mistral on your own data ðŸ¤™\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune the [Mistral 7B](https://github.com/mistralai/mistral-src) model - which outperforms Llama 2 13B on all tested benchmarks - ***on your own data!***\n",
    "\n",
    "## Watch the accompanying video walk-through [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)!\n",
    "\n",
    "I did this for **just one dollar ($1)** on an 1x A10G 24GB from Brev.dev (instructions below).\n",
    "\n",
    "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face ðŸ¤—.\n",
    "\n",
    "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
    "\n",
    "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9TytWkb3m15"
   },
   "source": [
    "#### Before we begin: A note on OOM errors\n",
    "\n",
    "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll).\n",
    "\n",
    "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Let's begin!\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n",
    "\n",
    "As you can see below, I have `notes.jsonl` for my `train_dataset` and `notes_validation.jsonl` for my `eval_dataset`.\n",
    "\n",
    "I used Exporter, a free local-only app, to export my Apple Notes to `.txt` files, and then I wrote a script to process each note into one `.jsonl` file. Note that for this script, ChatGPT can help out a LOT if you tell it how your data is currently formatted, how you'd like it to be formatted, and ask it to write a script in a certain language you know well (for any debugging) to do so. I also broke up my journal entries so the training sample vector length was smaller (see the discussion on `max_length` and the data visualization below). I broke it into pieces so that contexts were encapsulated entirely, since I did want the model to understand context about my life. My data were ultimately formatted as:\n",
    "\n",
    "```json\n",
    "{\"note\": \"journal-entry-for-model-to-predict\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-1\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-2\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset\n",
    "\n",
    "I used a GPU and dev environment from [brev.dev](https://brev.dev). The whole thing cost me $1 using a 1xA10G 24GB. Click the badge below to get your preconfigured instance:\n",
    "\n",
    "[![](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&diskStorage=256&name=mistral-finetune-own-data&file=https://github.com/brevdev/notebooks/raw/main/mistral-finetune-own-data.ipynb&python=3.10&cuda=12.0.1)\n",
    "\n",
    "A single A10G (as linked) with 24GB GPU Memory was enough for me. You may need more GPUs and/or Memory if your sequence max_length is larger than 512.\n",
    "\n",
    "Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10 and CUDA 12.0.1**; these should be preconfigured for you if you use the badge above) and click the \"Build\" button to build your verb container. Give this a few minutes.\n",
    "\n",
    "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
    "\n",
    "\n",
    "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T21:35:36.129435900Z",
     "start_time": "2024-01-16T21:34:41.121403500Z"
    },
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T21:28:59.697226300Z",
     "start_time": "2024-01-16T21:28:59.647929300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data \n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train.to_json('songs_train.jsonl', orient='records', lines=True)\n",
    "test.to_json('songs_validation.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:33:57.748429900Z",
     "start_time": "2024-01-16T22:33:56.039910Z"
    },
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train = [x for x in open('train_data.txt', 'rt').readlines()]\n",
    "test = [x for x in open('test_data.txt', 'rt').readlines()]\n",
    "\n",
    "# train_dataset = Dataset.from_list(train)\n",
    "# eval_dataset = Dataset.from_list(test)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### Accelerator\n",
    "\n",
    "Set up the Accelerator. I'm not sure if we really need this for a QLoRA given its [description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp) (I have to read more about it) but it seems it can't hurt, and it's helpful to have the code for future reference. You can always comment out the accelerator if you want to try without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:34:09.971271200Z",
     "start_time": "2024-01-16T22:34:07.911346800Z"
    },
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"975f16bcc5c0821264144a8460b0aed908261e55\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"tag-met-up-daddy\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"aeolin\"\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:34:13.944734500Z",
     "start_time": "2024-01-16T22:34:13.928735200Z"
    },
    "id": "f-fJR0MlQiTD"
   },
   "outputs": [],
   "source": [
    "def formatting_func(data):\n",
    "    #text = f\"[{example['ArtistNames']}]\\n\\r{example['Lyrics']}\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:34:18.781696700Z",
     "start_time": "2024-01-16T22:34:17.131265800Z"
    },
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ddff5ba3ca4e0a8539f7dcea8f11f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f728db7a414c4354a424b13a1aa747ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93fae5d64bf4dbba409edf06269384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98391af5095d4ea7a975d00ec2641b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:14.332442800Z",
     "start_time": "2024-01-16T22:08:14.117702600Z"
    },
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766451bf08c546949b9e547494658def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9031da43fff43ad861951df2f136764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99e302d29c491eafb7f9b66be0620a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f87e6c9d094c40aa39f3717986ea8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:17.658096400Z",
     "start_time": "2024-01-16T22:08:17.604001400Z"
    },
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "#tokenized_train_dataset = train.map(generate_and_tokenize_prompt)\n",
    "#tokenized_val_dataset = test.map(generate_and_tokenize_prompt)\n",
    "\n",
    "tokenized_train_dataset = map(generate_and_tokenize_prompt, train)\n",
    "tokenized_val_dataset = map(generate_and_tokenize_prompt, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:21.663377900Z",
     "start_time": "2024-01-16T22:08:20.649853600Z"
    },
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:25.091806600Z",
     "start_time": "2024-01-16T22:08:25.073576300Z"
    },
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 2000 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:26.532672500Z",
     "start_time": "2024-01-16T22:08:26.464454600Z"
    },
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = list(map(generate_and_tokenize_prompt2, train))\n",
    "tokenized_val_dataset = list(map(generate_and_tokenize_prompt2, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:29.775029800Z",
     "start_time": "2024-01-16T22:08:29.701571600Z"
    },
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 733, 16289, 3328, 2571, 311, 486, 401, 299, 884, 394, 377, 11695, 28711, 28792, 5760, 331, 11695, 28712, 28756, 28711, 28755, 3221, 28756, 28712, 28756, 28711, 28737, 3091, 2905, 272, 1069, 378, 27440, 28744, 28750, 28787, 28745, 28713, 6495, 298, 347, 28756, 28712, 28756, 28711, 28737, 3397, 369, 368, 347, 3411, 16719, 28744, 28750, 28787, 28745, 994, 19940, 528, 28725, 289, 1371, 28756, 28712, 28756, 28711, 28737, 1433, 15615, 28725, 315, 3091, 315, 541, 8547, 28725, 5982, 28756, 28712, 28756, 28711, 28737, 27440, 28744, 28750, 28787, 28745, 28719, 272, 8536, 369, 8108, 477, 272, 12230, 28756, 28712, 28756, 28711, 28737, 2203, 2598, 2905, 1671, 396, 5421, 28725, 5982, 28756, 28712, 28756, 28711, 28737, 541, 1484, 1346, 378, 16513, 28725, 1747, 27440, 28744, 28750, 28787, 28745, 28707, 625, 378, 2609, 528, 28725, 12176, 28725, 5982, 28756, 28712, 28756, 28711, 1313, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 378, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 264, 1043, 6671, 337, 28773, 1168, 28725, 5982, 28756, 28712, 28756, 28711, 28737, 1307, 298, 3383, 2079, 315, 27440, 28744, 28750, 28787, 28745, 28719, 559, 28773, 28756, 28712, 28756, 28711, 28737, 5102, 590, 1269, 378, 3081, 28725, 12176, 28756, 28712, 28756, 28711, 28737, 27440, 28744, 28750, 28787, 28745, 28719, 401, 299, 884, 394, 377, 28725, 590, 319, 266, 27440, 28744, 28750, 28787, 28745, 3229, 528, 28765, 299, 884, 394, 377, 28725, 590, 319, 266, 27440, 28744, 28750, 28787, 28745, 3229, 528, 28725, 5982, 28765, 299, 884, 394, 377, 28725, 590, 319, 266, 27440, 28744, 28750, 28787, 28745, 3229, 528, 20520, 586, 15139, 28756, 28712, 28756, 28711, 22829, 586, 3408, 28725, 12176, 28756, 28712, 28756, 28711, 12204, 28725, 5982, 28756, 28712, 28756, 28711, 28741, 8772, 28756, 28712, 28756, 28711, 12204, 28756, 28712, 28756, 28711, 1313, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 378, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 264, 1043, 6671, 1267, 28725, 5982, 325, 28762, 476, 28725, 5982, 28731, 1313, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 378, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 378, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 5982, 325, 28762, 1371, 28725, 5982, 28731, 1313, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 28725, 378, 27440, 28744, 28750, 28787, 28745, 28713, 750, 6671, 1267, 325, 1209, 28724, 4629, 2681, 28712, 28756, 28711, 28756, 28712, 28756, 28711, 28792, 2919, 311, 11695, 28712, 28756, 28711, 2438, 360, 16210, 16782, 28756, 28712, 28756, 28711, 28765, 299, 884, 394, 377, 28792, 28748, 16289, 4490, 28739, 12586, 1264, 7367, 1557, 28733, 19463, 548, 345, 1242, 548, 345, 434, 377, 548, 345, 28712, 8611, 548, 345, 1557, 4654, 2242, 28752, 2, 28705, 13, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:32.871677300Z",
     "start_time": "2024-01-16T22:08:31.382743300Z"
    },
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:36.596500Z",
     "start_time": "2024-01-16T22:08:36.541462100Z"
    },
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\n",
    "[K.I.Z]\n",
    "[Songtext zu Sangeeths ist BoXXXer]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhfq_Fa3m19"
   },
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:42.468090400Z",
     "start_time": "2024-01-16T22:08:39.655068100Z"
    },
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:46.738492500Z",
     "start_time": "2024-01-16T22:08:46.649320400Z"
    },
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:51.588553900Z",
     "start_time": "2024-01-16T22:08:51.577556800Z"
    },
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:54.374156200Z",
     "start_time": "2024-01-16T22:08:54.319764100Z"
    },
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:08:58.469397800Z",
     "start_time": "2024-01-16T22:08:57.520393700Z"
    },
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:09:00.361509Z",
     "start_time": "2024-01-16T22:09:00.341352300Z"
    },
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:09:05.726051Z",
     "start_time": "2024-01-16T22:09:05.717540300Z"
    },
    "id": "c_L1131GyRgo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 gpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    print(\"found >1 gpu\")\n",
    "else:\n",
    "    print(f\"found {torch.cuda.device_count()} gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:09:06.802831900Z",
     "start_time": "2024-01-16T22:09:06.790317400Z"
    },
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:23:56.418597Z",
     "start_time": "2024-01-16T22:17:35.578817Z"
    },
    "id": "jq0nX33BmfaC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/klessafloria@edu.local/Projects/TagMeUpDaddy/wandb/run-20240409_173256-71igsfba</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolin/tag-met-up-daddy/runs/71igsfba' target=\"_blank\">mistral-tag-me-up-daddy-2024-04-09-17-32</a></strong> to <a href='https://wandb.ai/aeolin/tag-met-up-daddy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolin/tag-met-up-daddy' target=\"_blank\">https://wandb.ai/aeolin/tag-met-up-daddy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolin/tag-met-up-daddy/runs/71igsfba' target=\"_blank\">https://wandb.ai/aeolin/tag-met-up-daddy/runs/71igsfba</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klessafloria@edu.local/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/500 00:11 < 1:35:26, 0.09 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 37\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/trainer.py:1849\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2193\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2196\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2199\u001b[0m ):\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/trainer.py:3128\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3128\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3131\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/trainer.py:3151\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3150\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3151\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/peft/peft_model.py:1290\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1289\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1290\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:178\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:1032\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1043\u001b[0m         hidden_states,\n\u001b[1;32m   1044\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1049\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/checkpoint.py:482\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         )\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    485\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    486\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/checkpoint.py:261\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 261\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:655\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    653\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    654\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 655\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    658\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:474\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 474\u001b[0m     output \u001b[38;5;241m=\u001b[39m lora_B(\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_dora(x, lora_A, lora_B, scaling, active_adapter)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"tag-me-up-daddy\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        fp16=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \" The following is a note by Eevee the Dog, which doesn't share anything too personal: # \"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to me on [X](https://x.com/harperscarroll) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
